{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1c022988-cc93-4b55-a8fb-d0472e9f3f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3132ba57-0c32-471b-9512-61cf602b83ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.cond(\n",
    "        tf.image.is_jpeg(image),\n",
    "        lambda: tf.image.decode_jpeg(image, channels=3),\n",
    "        lambda: tf.image.decode_png(image, channels=3))\n",
    "    image = tf.image.resize(image, (56, 56))\n",
    "    image = (image - 127.5) / 127.5  # [-1, 1]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "926fd14e-5c34-4d1f-a809-c15f62d0f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e5d5acd8-f6e7-4c0d-89d0-007669290b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normalized_dataset(path):\n",
    "\n",
    "    image_samples_path = list()\n",
    "    class_count = 1\n",
    "    max_samples = -1\n",
    "    max_classes = -1\n",
    "    for class_dir in os.listdir(path):\n",
    "        img_sample_count = 1\n",
    "        full_class_dir = os.path.join(path, class_dir)\n",
    "        for image_name in os.listdir(full_class_dir):\n",
    "            \n",
    "            full_image_name = os.path.join(full_class_dir, image_name)\n",
    "            image_samples_path.append(full_image_name)\n",
    "            \n",
    "            if img_sample_count == max_samples:\n",
    "                break\n",
    "\n",
    "            img_sample_count += 1\n",
    "\n",
    "        if class_count == max_classes:\n",
    "            break\n",
    "\n",
    "        class_count += 1\n",
    "\n",
    "    random.shuffle(image_samples_path)\n",
    "\n",
    "    with open('image_list.txt', 'w') as f:\n",
    "        for item in image_samples_path:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    scene_dataset = tf.data.Dataset.from_tensor_slices(image_samples_path)\n",
    "    scene_dataset = scene_dataset.map(load_and_preprocess_image)\n",
    "    batch_size = 256\n",
    "    scene_dataset = scene_dataset.batch(batch_size)\n",
    "\n",
    "    return scene_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1e59e57d-01df-4a0d-a2cf-8d21078a1a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\SKOLA\\\\FAKS\\\\RI-Logo-generation-GAN\\\\src'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a3a3d48c-9f2b-466e-8fa5-aab14d1ebac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dobar\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    f = open(\"../data/Amazon/Amazon logo2.jpg\")\n",
    "except FileNotFoundError:\n",
    "    print('Error')\n",
    "else:\n",
    "    print('Dobar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "45a1fb8d-552f-49a3-bd39-f03edc9ea1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=TensorSpec(shape=(None, 56, 56, 3), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_dataset = load_normalized_dataset(\"../data\")\n",
    "scene_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "6193ea34-451c-4b7b-a2b0-9159aa333750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(units=7 * 7 * 256, use_bias=False, input_shape=(100,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((7, 7, 256)),\n",
    "\n",
    "        layers.Conv2DTranspose(filters=128, kernel_size=(5, 5), strides=(1, 1), padding=\"same\", use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        layers.Conv2DTranspose(filters=256, kernel_size=(5, 5), strides=(2, 2), padding=\"same\", use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        layers.Conv2DTranspose(filters=32, kernel_size=(5, 5), strides=(2, 2), padding=\"same\", use_bias=False),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "\n",
    "        layers.Conv2DTranspose(filters=3, kernel_size=(5, 5), strides=(2, 2), padding=\"same\", use_bias=False,\n",
    "                               activation=\"tanh\"),\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "94c4f925-1283-4a49-b2c4-163e6788949f",
   "metadata": {},
   "outputs": [],
   "source": [
    " gen_model = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7ef5d917-5bb9-4f67-be18-2831455499bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b17f0735-8abd-4c14-938f-b3c7ce97d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_optimizer():\n",
    "    return tf.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cf2080b1-fb3e-47de-99a1-2131625c9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_optimizer = generator_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "11b6c426-3d76-4bda-a2c2-8a7859305860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2), padding='same',\n",
    "                      input_shape=[56, 56, 3]),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(rate=0.3),\n",
    "\n",
    "        layers.Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(rate=0.3),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(units=1),\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "1c809ca4-f41a-46e3-a08f-e7e0c964df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_model = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f2e029b9-9c2e-40bf-abc7-257c3d3710d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output,\n",
    "                       fake_output):\n",
    "    cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f9b5745d-b565-498a-a124-b5d7be9cd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_optimizer():\n",
    "    return tf.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "55f40edd-2e97-4598-a10e-5caec0fe524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_optimizer = discriminator_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2c3cc675-03a9-4130-b9aa-e20cf78b2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, gen_model, gen_optimizer, dis_model, dis_optimizer, gen_loss_metric, dis_loss_metric):\n",
    "    noise = tf.random.normal([256, 100])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = gen_model(noise, training=True)\n",
    "\n",
    "        real_output = dis_model(images, training=True)\n",
    "        fake_output = dis_model(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, gen_model.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, dis_model.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, gen_model.trainable_variables))\n",
    "    dis_optimizer.apply_gradients(zip(gradients_of_discriminator, dis_model.trainable_variables))\n",
    "\n",
    "    gen_loss_metric(gen_loss)\n",
    "    dis_loss_metric(disc_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b66d9337-617a-4e4d-9eea-60b396d9bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(real_images, gen_model, dis_model):\n",
    "    random_seed = tf.random.normal([256, 100])\n",
    "    fake_images = gen_model(random_seed, training=False)\n",
    "\n",
    "    real_dis_prediction = dis_model(real_images)\n",
    "    fake_dis_prediction = dis_model(fake_images)\n",
    "\n",
    "    correct = len(real_dis_prediction[real_dis_prediction >= 0.0])\n",
    "    wrong = len(real_dis_prediction[real_dis_prediction < 0.0])\n",
    "    real_dis_acc = float(correct) / float(correct + wrong)\n",
    "\n",
    "    correct = len(fake_dis_prediction[fake_dis_prediction < 0.0])\n",
    "    wrong = len(fake_dis_prediction[fake_dis_prediction >= 0.0])\n",
    "    fake_dis_acc = float(correct) / float(correct + wrong)\n",
    "\n",
    "    combined_dis_acc = (real_dis_acc + fake_dis_acc) / 2\n",
    "\n",
    "    return real_dis_acc, fake_dis_acc, combined_dis_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8dab5ba2-a644-4c1c-98fe-d2caf4badcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_and_accuracy(train_summary_writer, gen_loss_metric, epoch, dis_loss_metric, real_dis_acc, fake_dis_acc, combined_dis_acc):\n",
    "    with train_summary_writer.as_default():\n",
    "        with tf.name_scope('Loss'):\n",
    "            tf.summary.scalar('Generator', gen_loss_metric.result(), step=epoch)\n",
    "            tf.summary.scalar('Discriminator', dis_loss_metric.result(), step=epoch)\n",
    "\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            tf.summary.scalar('Real Discriminator', real_dis_acc, step=epoch)\n",
    "            tf.summary.scalar('Fake Discriminator', fake_dis_acc, step=epoch)\n",
    "            tf.summary.scalar('Combined Discriminator', combined_dis_acc, step=epoch)\n",
    "\n",
    "    gen_loss_metric.reset_states()\n",
    "    dis_loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "eb1d4e09-daeb-44f8-8057-2b846b376e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_and_biases(train_summary_writer, gen_model, epoch, dis_model):\n",
    "    with train_summary_writer.as_default():\n",
    "        with tf.name_scope('Generator'):\n",
    "            with tf.name_scope('Weights'):\n",
    "                for gen_layer in gen_model.layers:\n",
    "                    try:\n",
    "                        tf.summary.histogram(gen_layer.name, gen_layer.get_weights()[0], step=epoch)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        with tf.name_scope('Discriminator'):\n",
    "            for dis_layer in dis_model.layers:\n",
    "                try:\n",
    "                    with tf.name_scope('Weights'):\n",
    "                        tf.summary.histogram(dis_layer.name, dis_layer.get_weights()[0], step=epoch)\n",
    "                    with tf.name_scope('Biases'):\n",
    "                        tf.summary.histogram(dis_layer.name, dis_layer.get_weights()[1], step=epoch)\n",
    "                except Exception:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ff742037-a540-456d-a922-f6c6808775be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(gen_model, epoch, train_summary_writer):\n",
    "    test_input = tf.random.normal([16, 100])\n",
    "    predictions = None\n",
    "  #  if (epoch % 100 == 0):\n",
    "    predictions = gen_model(test_input, training=False)\n",
    "\n",
    "   # if epoch % 100 == 0:\n",
    "    for i in range(predictions.shape[0]):\n",
    "        img = np.array(predictions[i]) * 127.5 + 127.5\n",
    "        img = img.astype(np.uint8, copy=False)\n",
    "        plt.imsave('../res/image_at_epoch_{:05d}_{:05d}.png'.format(epoch, i), img)\n",
    "\n",
    "   # if epoch % 100 == 0:\n",
    "    with tf.name_scope(\"Samples\"):\n",
    "        with train_summary_writer.as_default():\n",
    "            for i in range(predictions.shape[0]):\n",
    "                img = np.array(np.array(predictions[i]) * 0.5 + 0.5).astype(np.float32)\n",
    "                img = np.reshape(img, (1, 56, 56, 3))\n",
    "                tf.summary.image('image_{:03d}.png'.format(i), img, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3a2a43a7-95ef-459e-b6f7-9047dd28e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(real_image_dataset, last_epoch, epochs, gen_model, gen_optimizer, dis_model, dis_optimizer):\n",
    "    gen_loss_metric = keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    dis_loss_metric = keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    train_log_dir = '../log'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    for epoch in range(last_epoch, epochs):\n",
    "        for image_batch in real_image_dataset:\n",
    "            train_step(image_batch, gen_model, gen_optimizer, dis_model, dis_optimizer, gen_loss_metric, dis_loss_metric)\n",
    "            real_dis_acc, fake_dis_acc, combined_dis_acc = test_step(image_batch, gen_model, dis_model)\n",
    "            loss_and_accuracy(train_summary_writer, gen_loss_metric, epoch, dis_loss_metric, real_dis_acc, fake_dis_acc, combined_dis_acc)\n",
    "            weights_and_biases(train_summary_writer, gen_model, epoch, dis_model)\n",
    "        print(\"Generisem sliku\")\n",
    "        save_image(gen_model, epoch + 1, train_summary_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5a909-bc61-4fb3-b4c9-957a8e306ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n",
      "Generisem sliku\n"
     ]
    }
   ],
   "source": [
    "train(real_image_dataset=scene_dataset, last_epoch=0,epochs=100,gen_model=gen_model, gen_optimizer=gen_optimizer, dis_model=dis_model,\n",
    "            dis_optimizer=dis_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a0172-f20e-41fe-a0e5-1b9f04c1eaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065517f5-fb6c-47a0-8f90-0518d9c645bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4453e08e-1dd5-4301-b38e-90a09f2d1ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
